
# Groupe:

- AMAR HASNAOUI, AMINE NAIT SIDHOUM, AYMEN MEZIANE

# Documentation Technique : Projet BDF3


### Pr√©sentation du projet

L‚Äôobjectif de ce projet est de concevoir et mettre en place une architecture Big Data en m√©daillon (Bronze / Silver / Gold) afin de traiter et valoriser un jeu de donn√©es de transactions. Cette architecture permettra d‚Äôassurer un traitement structur√©, fiable et √©volutif des donn√©es, depuis l‚Äôingestion brute jusqu‚Äô√† leur exploitation analytique.

---

### Architecture 

### 1. Mise en place
- Nous avons installer la meme image que le professeur nous a fournie pour realiser le projet **BDF1** avec hadoop, spark et hive.
- une image mysql
- Dans le rendu bdf3 vous allez trouver des sous dossiers, **Traitement**, **ML**, **Source**, **lib**, **logs**.
- Traitement contient les trois fichiers : **feeder.py** | **processor.py** | **gold.py**
- ML contient le fichier : **detect_fraud.py**
- Source contient les fichier Source
- lib contient je jbdc mysql
- logs dossier pour stocker les logs 
- Dans notre conteneur spark √† l'aide de docker cp nous avons transferer c'est dossiers dans un dossier **projet_bdf3**                                                      

[capture d'√©cran tree sur conteneur](https://photos.app.goo.gl/pz4v6XaFMUPHQUKBA)           

# Pipeline de Donn√©es vers Zone Bronze ‚Äì Composant Feeder

## 1. Objectif du Script

Ce script Spark en Python a pour but de :
- **Extraire** les donn√©es brutes de diff√©rentes sources (MySQL, CSV, JSON).
- **Nettoyer, transformer** et **partitionner** ces donn√©es.
- **Stocker** les donn√©es format√©es dans la **zone Bronze** du data lake, en format **Parquet partitionn√©** par date (`year/month/day`).

Cette √©tape est appel√©e **"Feeder"**, car elle alimente la zone Bronze √† partir des sources brutes.

---

## 2. Architecture du Script (Structure orient√©e objet)

Le script est structur√© en plusieurs **classes** qui s√©parent clairement les responsabilit√©s :

| Classe               | R√¥le                                                                 |
|----------------------|----------------------------------------------------------------------|
| `SparkSessionManager` | Cr√©e et configure la session Spark.                                  |
| `LoggerManager`       | Configure les logs pour le suivi des traitements.                   |
| `BaseProcessor`       | Contient les m√©thodes et dates communes aux autres classes.         |
| `MySQLProcessor`      | Lit les donn√©es `transactions` depuis MySQL par batchs.              |
| `CSVProcessor`        | Transforme et charge les fichiers CSV (`cards`, `users`).            |
| `JSONProcessor`       | Transforme et charge les fichiers JSON (`mcc_codes`, `train_labels`).|

---

## 3. Traitement par Source

### a) Donn√©es SQL (transactions)

- **Source** : base MySQL `Source`, table `transactions`.
- **Traitement** :
  - Lecture **par lot de 4 millions** d‚ÄôIDs pour √©viter les surcharges m√©moire.
  - Conversion de la colonne `situation_date` au format date.
  - Ajout de colonnes `year`, `month`, `day` pour la partition.
  - Sauvegarde en **Parquet partitionn√©** dans `/Bronze/transactions/`.
- **Format final** : Parquet, partitionn√© par `year`, `month`, `day`.

---

### b) Donn√©es CSV (`cards`, `users`)

- **Source** : fichiers plats CSV.
- **Traitement** :
  - Lecture avec en-t√™te (`header=True`) et s√©parateur `,`.
  - Conversion directe au format Parquet.
  - Sauvegarde partitionn√©e dans `/Bronze/<table>/year=YYYY/month=MM/day=DD/`.
- **Format final** : Parquet partitionn√©.

---

### c) Donn√©es JSON (`mcc_codes`, `train_fraud_labels`)

- `mcc_codes.json, train_fraud_labels.json` : fichier au format `cl√©:valeur`, transform√© en deux colonnes.
- **Traitement** :
  - Chargement des objets JSON.
  - Sauvegarde partitionn√©e en Parquet comme les autres.

---

## 4. Bonnes pratiques de **Data Management** appliqu√©es

| Bonne pratique                            | Application dans le script                                          |
|------------------------------------------|---------------------------------------------------------------------|
| üîπ **Partitionnement temporel**           | `year/month/day` utilis√© dans tous les formats pour requ√™tes efficaces. |
| üîπ **Format optimis√©**                    | Utilisation du **format Parquet** pour la compression + performance.  |
| üîπ **Modularit√©**                         | Le script est orient√© objet avec des classes ind√©pendantes.         |
| üîπ **Logs et monitoring**                 | Chaque √©tape est logg√©e avec niveau `INFO` ou `ERROR`.              |
| üîπ **Gestion m√©moire**                    | Utilisation de `.cache()`, `.unpersist()`, `gc.collect()`.          |
| üîπ **Lecture incr√©mentale SQL**           | Lecture par **batch** via des plages d‚ÄôID.                          |
| üîπ **S√©paration des responsabilit√©s**     | Code d√©coup√© en classes sp√©cialis√©es.       |

---

## 5. Organisation des Donn√©es ‚Äì Zone Bronze

Les donn√©es sont organis√©es dans `/projet_bdf3/Bronze/<table>/year=YYYY/month=MM/day=DD/` :

Exemple : [capture d'√©cran tree](https://photos.app.goo.gl/fnxYBVSyfAE3pcMU9)



Ce script ex√©cute le traitement des donn√©es issues du niveau **Bronze** pour les transformer en **niveau Silver**.  
Il applique des √©tapes de **normalisation**, **nettoyage**, **typage**, et enregistre les donn√©es :

- dans **Hive** (via des tables partitionn√©es)  
- et dans des fichiers **Parquet** sur le **HDFS local**

---

# Pipeline de Donn√©es vers Zone Silver ‚Äì Composant processor

##  Architecture du script

### Classes

#### `PipelineProcessor`

Classe principale orchestrant les traitements Spark pour les diff√©rentes tables Bronze.

---

### M√©thodes internes

- `get_latest_local_path(base_path: str) -> str | None`  
  ‚Üí Retourne le chemin le plus r√©cent (par date) dans la hi√©rarchie `year=/month=/day=`.

- `extract_date_parts_from_path(path: str) -> tuple[int, int, int]`  
  ‚Üí Extrait `year`, `month`, `day` √† partir d'un chemin format√©.

- `init_spark()`  
  ‚Üí Initialise une `SparkSession` avec support Hive (via `hive.metastore.uris`).

---

##  M√©thodes de traitement par table

### `process_transactions()`

- **Source** : `/projet_bdf3/Bronze/transactions/`

- **Nettoyage** :
  - Nettoyage des colonnes `amount` ‚Üí `amount_usd`, en enlevant `$`
  - Conversion des types : `id`, `zip`, `mcc`, etc.

- **√âcriture** :
  - Hive : `silver.transactions`
  - Parquet : `/projet_bdf3/Silver/transactions/`

---

### `process_cards()`

- **Source** : `/projet_bdf3/Bronze/cards/`

- **Nettoyage** :
  - Conversion de cha√Ænes `"YES"` / `"NO"` en bool√©ens (`has_chip`, `card_on_dark_web`)
  - `credit_limit` ‚Üí `credit_limit_usd` en enlevant `$`
 
- **√âcriture** :
  - Hive : `silver.cards`
  - Parquet : `/projet_bdf3/Silver/cards/`

---

### `process_users()`

- **Source** : `/projet_bdf3/Bronze/users/`

- **Nettoyage** :
  - `gender` : `"Male"` / `"Female"` ‚Üí `"M"` / `"F"`
  - `per_capita_income` ‚Üí `per_capita_income_usd`

- **√âcriture** :
  - Hive : `silver.users`
  - Parquet : `/projet_bdf3/Silver/users/`

---

### `process_mcc_codes()`

- **Source** : `/projet_bdf3/Bronze/mcc_codes/`

- **Nettoyage** :
  - `mcc_code` converti en entier

- **√âcriture** :
  - Hive : `silver.mcc`
  - Parquet : `/projet_bdf3/Silver/mcc_codes/`

---

## Logs

- Fichier log : `/projet_bdf3/logs/pipeline_processor.log`
-  Les logs sont aussi affich√©s dans la console (via `StreamHandler`)
- Log des √©tapes :
  - Lecture
  - Normalisation
  - √âcriture
  - Dur√©e du traitement

---

##  Nettoyage m√©moire

Chaque `DataFrame` est :

- `unpersist()`
- Supprim√© via `del df`
- Collect√© manuellement avec `gc.collect()`

[capture d'√©cran tree silver](https://photos.app.goo.gl/DH9rnWF7uLFoUdsb7)                                                      
[capture d'√©cran Hive](https://photos.app.goo.gl/JYL752rJfPUr97gc6)                                                            
[capture d'√©cran Hive](https://photos.app.goo.gl/GbAxfhGBPBGG8dHa9)                                                              
[capture d'√©cran Hive](https://photos.app.goo.gl/A715c74MuZftsxAZA)                                                            
[capture d'√©cran Hive](https://photos.app.goo.gl/j3wcvKD15ypAp2oo8)

---

# Pipeline de Donn√©es vers Zone Gold ‚Äì Composant gold

## Objectif

Le script Spark vise √† transformer des donn√©es Silver (stock√©es dans Hive) en plusieurs **datamarts** m√©tier dans la zone Gold (MySQL). Chaque datamart r√©pond √† une probl√©matique analytique sp√©cifique li√©e aux transactions bancaires, √† la fraude, aux cartes et aux clients.

---

## Architecture de la classe `Datamarts`

class Datamarts:
Cette classe contient 5 m√©thodes qui encapsulent chacune un datamart sp√©cifique. Chaque m√©thode prend un ou plusieurs DataFrames en entr√©e, effectue des transformations Spark (groupBy, joins, aggregations), et retourne un DataFrame pr√™t √† √™tre √©crit dans MySQL.

## D√©tail des Datamarts cr√©√©s

1. dm_fraude_detection(df: DataFrame)
But : Identifier le volume de fraudes par ville.
Traitement :
    - Groupement par merchant_city et is_fraud.
    - Agr√©gation : nombre total de transactions (count(*)).
    - R√©sultat : [capture d'√©cran Mysql](https://photos.app.goo.gl/6pJXUCoM4L5dmbmb8)

2. dm_clients(df_users: DataFrame, df_cards: DataFrame)
But : Analyser le profil des clients avec leurs cartes bancaires.
Traitement :
    - Jointure entre users et cards (via client_id).
    - Colonnes extraites : user_id, gender, credit_limit_usd, num_credit_cards.
    - R√©sultat : [capture d'√©cran Mysql](https://photos.app.goo.gl/qC85PiCJjiVHELzY7)

3. dm_transactions_par_region(df: DataFrame)
But : Observer les montants de transactions par √âtat.
Traitement :
    - Groupement par merchant_state.
    - Agr√©gation sur la somme des amount_usd.
    - R√©sultat : [capture d'√©cran Mysql](https://photos.app.goo.gl/Z14rp6ALpzWBTtDA9)

4. dm_cartes(df: DataFrame)
But : Analyser la r√©partition des cartes par type et marque.
Traitement :
    - Groupement par card_brand et card_type.
    - Comptage du nombre de cartes.
    - R√©sultat : [capture d'√©cran Mysql](https://photos.app.goo.gl/z89beEZEAo8XeuQS8)

5. dm_mcc_categories(df_trans: DataFrame, df_mcc: DataFrame)
But :Comprendre les transactions par cat√©gorie MCC (Merchant Category Code).
Traitement :
    - Jointure avec la table mcc_codes sur le champ mcc.
    - Groupement par description.
    - Comptage du nombre de transactions.
    - R√©sultat : [capture d'√©cran Mysql](https://photos.app.goo.gl/XUUM8jf482Z4boYW6)

## Stockage final : MySQL (Zone Gold)
Toutes les sorties des datamarts sont √©crites dans une base MySQL gold, en mode overwrite, via JDBC

```pyton
df.write.mode("overwrite").jdbc(url=MYSQL_URL, table=table_name, properties=MYSQL_PROPERTIES)
```

## Points de clart√©
| Crit√®re                                  | √âvaluation                                                             |
| ---------------------------------------- | ---------------------------------------------------------------------- |
| **Structure claire** des datamarts       |  Classe d√©di√©e `Datamarts`, m√©thodes bien nomm√©es                    |
| **Lisibilit√©** des transformations Spark |  Fonctions explicites avec `groupBy`, `agg`, `join`                  |
| **Objectif m√©tier identifiable**         |  Chaque datamart r√©pond √† un besoin analytique concret               |
| **S√©paration logique des √©tapes**        |  Lecture, transformation, √©criture s√©par√©es proprement               |
| **Commentaires / Logs**                  |  Logs pr√©sents  |

---
# Execution
```
spark-submit \
  --jars lib/mysql-connector-j-8.3.0.jar \
  --master yarn \
  --deploy-mode cluster \
  --num-executors 2 \
  --executor-memory 2G \
  --driver-memory 2G \
  --executor-cores 2 \
  Traitement/"traitement".py
```
√âtant donn√© que ma machine dispose de 8 Go de RAM et 4 c≈ìurs, j‚Äôai ajust√© les ressources comme suit pour assurer un traitement parall√®le tout en √©vitant la saturation m√©moire :

- num-executors 2 : J‚Äôai choisi de lancer 2 ex√©cuteurs pour permettre le parall√©lisme sans d√©passer les ressources disponibles.
- executor-cores 2 : Chaque ex√©cuteur utilise 2 c≈ìurs, ce qui utilise l'ensemble des 4 c≈ìurs disponibles (2 ex√©cuteurs √ó 2 c≈ìurs).
- executor-memory 2G : J‚Äôalloue 2 Go de RAM √† chaque ex√©cuteur. Avec 2 ex√©cuteurs, cela fait 4 Go utilis√©s par les ex√©cuteurs.
- driver-memory 2G : Le driver re√ßoit √©galement 2 Go, ce qui fait un total de 6 Go allou√©s.
- Je garde une marge de 2 Go libre pour le syst√®me d‚Äôexploitation et les processus Spark internes, ce qui est important pour √©viter les erreurs OutOfMemory.
                                            

[capture d'√©cran yarn](https://photos.app.goo.gl/ZfTpgZ7wHHwR7x7cA)                                                                 
[capture d'√©cran spark ui](https://photos.app.goo.gl/iuJZBPMs73uEHm119)

---

# Machine Learning

## D√©tection de Fraude avec Spark et GBTClassifier

### Objectif

Ce module vise √† entra√Æner un mod√®le de machine learning pour **d√©tecter les fraudes bancaires** √† partir des transactions pr√©sentes dans la zone Silver de notre Data Lake. L‚Äôapproche est supervis√©e, avec un apprentissage √† partir des donn√©es labellis√©es et un √©quilibrage des classes.

---

### Technologies utilis√©es

- **Apache Spark** (PySpark) pour le traitement distribu√©  
- **Hive** pour le stockage des tables Silver  
- **MLlib** (Spark ML) pour l'entra√Ænement du mod√®le  
- **GBTClassifier** (Gradient Boosted Trees)  

---

### Pipeline de traitement

1. **Chargement des donn√©es depuis Hive** :
   - `silver.transactions`
   - `silver.train_fraud_labels`
   - `silver.cards`
   - `silver.mcc_codes`

2. **Pr√©paration des donn√©es** :
   - Jointure des tables via les cl√©s `id`, `card_id`, `mcc`
   - Conversion des colonnes n√©cessaires (`has_chip`, `fraud_label`) en types num√©riques
   - Filtrage pour ne conserver que les transactions ayant un label

3. **√âchantillonnage √©quilibr√©** :
   - Toutes les fraudes (`label = Yes`)
   - √âchantillon al√©atoire de 10 % des non-fraudes (`label = No`)
   - Objectif : limiter le biais de classe majoritaire

4. **Traitement des variables cat√©gorielles** :
   - Colonnes : `merchant_state`, `card_brand`, `card_type`, `has_chip`, `description`
   - Transformation avec `StringIndexer` 

5. **Assemblage des features** :
   - Num√©riques : `amount_usd`, `credit_limit_usd`
   - Cat√©gorielles index√©es
   - Utilisation de `VectorAssembler`

6. **Entra√Ænement du mod√®le** :
   - Mod√®le : `GBTClassifier` (30 it√©rations, 200 bins max)
   - Split 80/20 pour train/test
   - V√©rification que l'ensemble d'entra√Ænement n'est pas vide

7. **√âvaluation** :
   - M√©trique : AUC (Area Under ROC Curve) 
   - Pr√©dictions sur le jeu de test

8. **Sauvegarde du mod√®le** :
   - R√©pertoire : `ML/saved_model_balanced`
   - Format Spark ML

---

### R√©sultat

- Le mod√®le a √©t√© entra√Æn√© avec succ√®s sur un √©chantillon √©quilibr√©.  
- Une AUC a √©t√© calcul√©e = 0.98 pour √©valuer la capacit√© du mod√®le √† distinguer les fraudes.  
- Le mod√®le a √©t√© sauvegard√© pour r√©utilisation dans la phase de scoring ou d'inf√©rence.  

[capture d'√©cran entrainement model](https://photos.app.goo.gl/daFVVCfpxm7fZ4Gb7)

---

### Test du model
Le model a √©t√© test√© sur deux transactions via code python **test_model.py**.
[capture d'√©cran test model](https://photos.app.goo.gl/rr4hpUbmcqPMxXLe6)

---

# Documentation API ‚Äì Acc√®s s√©curis√© aux donn√©es MySQL (couche Gold)

## Contexte

Dans le cadre de notre architecture orient√©e donn√©es, cette API l√©g√®re permet d‚Äôex√©cuter dynamiquement des requ√™tes SQL sur la base MySQL de la **zone gold**, tout en assurant un **acc√®s s√©curis√©**, une **tra√ßabilit√© des connexions**, et un **enregistrement des r√©sultats**.

---

## Objectifs

- Fournir une interface REST pour interroger la base de donn√©es MySQL.
- Authentifier les utilisateurs via un token unique.
- Consigner toutes les requ√™tes dans un fichier de logs.
- Permettre une exportation automatique des r√©sultats au format CSV.

---

## Technologies utilis√©es

| Composant         | D√©tail                                   |
|-------------------|-------------------------------------------|
| **Framework**     | Django  |
| **Base de donn√©es** | MySQL (zone gold, conteneur Docker)      |
| **Authentification** | Token UUID g√©n√©r√© dynamiquement         |
| **Fichier de logs** | `logs/access.log` (texte brut horodat√©) |
| **Export**         | R√©sultats enregistr√©s en CSV dans `query_results/` |
| **Outils de test** | Postman                          |

---

## Authentification

### Route : `/login` (POST)

Permet d'obtenir un **token d'acc√®s** unique pour l‚Äôutilisateur autoris√© `aminamar`.

### Requ√™te

```json
POST /login
Content-Type: application/json

{
  "username": "aminamar",
  "password": "admin123"
}
```
### R√©ponse (succ√®s)

```json
{
  "message": "Connexion r√©ussie",
  "token": "1d8a9e10-4d30-4f43-9b12-2b8f3f3d5478"
}
```
[capture d'√©cran token](https://photos.app.goo.gl/8VGCBze9m29E8qYt9)

### R√©ponses d‚Äôerreur possibles

- `403 FORBIDDEN` : Vous n'avez pas acc√®s.
- `401 UNAUTHORIZED` : Mot de passe incorrect.

[capture d'√©cran utilisateur incorect](https://photos.app.goo.gl/6LezK1BhQuLvjnxp9)              [capture d'√©cran mot de passe incorect](https://photos.app.goo.gl/UyDBDJpMxiL3WJSr8)

---

## Ex√©cution de requ√™tes SQL

### Route : `/query` (POST)

Permet de soumettre une requ√™te SQL (type `SELECT`) √† la base de donn√©es MySQL.

### En-t√™te requis

```http
Authorization: Bearer <your_token_here>
```
[capture d'√©cran token](https://photos.app.goo.gl/r1nYL8QjnHnwb9Bw5)

### Requ√™te

```json
POST /query
Content-Type: application/json

{
  "query": "SELECT * FROM dm_cartes"
}
```

### R√©ponse (succ√®s)

```json
{
  "message": "Query executed and saved to CSV successfully.",
  "results": [
    {
      "id": 1,
      "amount": 500,
      "date": "2023-01-01"
    }
  ],
  "file": "query_results/query_2025-05-30_17-43-21.csv"
}
```
[capture d'√©cran reultat requete](https://photos.app.goo.gl/fiGTfLS6meLk5vzJA)                     
[capture d'√©cran reultat fichier](https://photos.app.goo.gl/imwEPFwNqb1sZKFP6)                     
[capture d'√©cran reultat requete](https://photos.app.goo.gl/aJw3UWGVX4aJaBVVA)                     
[capture d'√©cran reultat fichier](https://photos.app.goo.gl/X2xxd2mA7sDcXe5e8)
### R√©ponses d‚Äôerreur possibles

- `400 BAD REQUEST` : Requ√™te SQL vide.
- `400 BAD REQUEST` : Erreur SQL lev√©e (malform√©e ou non autoris√©e).
- `403 FORBIDDEN` : Token manquant ou invalide.

[capture d'√©cran Requ√™te SQL vide](https://photos.app.goo.gl/LK2fzRqY1rEVkSTF9)                     
[capture d'√©cran Erreur SQL](https://photos.app.goo.gl/qJa5czKLqVn6Y2QWA)                     
[capture d'√©cran Token manquant](https://photos.app.goo.gl/RNYCcKuStbZi3MR99)
---

##Ô∏è Journalisation des acc√®s

Chaque requ√™te `/query` est consign√©e dans le fichier local suivant :

```
logs/access.log
```

---

##  S√©curit√© & validation

- *V√©rification du token** via d√©corateur `@token_required`.
- Export automatique des r√©sultats en CSV dans un dossier s√©curis√©.
- Traitement des erreurs avec des r√©ponses structur√©es c√¥t√© client.

---

# Documentation ‚Äì Visualisation et Recommandations (2/2)
## Objectif
Analyser les transactions bancaires par zone g√©ographique, genre, type de carte, ville marchande, et cat√©gorie de d√©pense afin d‚Äôidentifier les tendances, les comportements clients, et proposer des axes d‚Äôoptimisation.

1. Visualisation 1 ‚Äì Analyse g√©ographique et par ville marchande Carte des √âtats-Unis (par merchant_state)

    Repr√©sente la somme de total_usd par √âtat.
    Une √©chelle de couleurs montre les √âtats avec les volumes de transaction les plus √©lev√©s (plus fonc√© = plus √©lev√©).
    Bar chart ‚Äì Nombre de transactions par merchant_city
    La ville "ONLINE" est largement dominante avec pr√®s de 2M de transactions, bien devant Houston, Miami, Brooklyn, etc.
    Cela sugg√®re une forte part des transactions.

## Recommandations
Segmentation : cr√©er une cat√©gorie sp√©cifique pour les transactions "ONLINE" afin d‚Äôen distinguer l‚Äôanalyse.
Focus marketing : investir dans les villes physiques √† fort volume (Houston, Miami, etc.) pour stimuler encore plus l'activit√©.
Analyse par √âtat : les √âtats les plus fonc√©s (ex. : NY, CA, FL) m√©ritent une attention particuli√®re pour l‚Äôallocation de ressources.

2. Visualisation 2 ‚Äì Analyse par genre, carte, type de d√©penses, et localisation Cr√©dit disponible par genre (credit_limit_usd)

    R√©partition assez √©quilibr√©e entre les femmes (F) et les hommes (M).
    L√©g√®re avance du cr√©dit total disponible pour les femmes.
    Nombre de transactions par description
    Cat√©gories principales : √©piceries, stations-service, restaurants, pharmacies, etc.
    R√©partition logique avec les besoins de consommation courante.
    Mastercard domine, suivie de Visa.
    D√©bit est le type de carte le plus r√©pandu, devant le cr√©dit et le pr√©pay√©.
    total_usd par √âtat marchand
    L'√âtat Californie  affiche un total tr√®s √©lev√©.
    Suivi par New York, etc.

## Recommandations
Produit bancaire cibl√© : promouvoir davantage les cartes de cr√©dit (moins utilis√©es) pour augmenter les marges bancaires.
Analyse d√©taill√©e des d√©penses : les √©piceries et services alimentaires concentrent le plus de transactions ‚Üí opportunit√© de partenariats.
√âtude de genre : comprendre pourquoi les femmes ont un cr√©dit disponible l√©g√®rement plus √©lev√© ‚Äì potentiel d‚Äôapprofondissement.















---
